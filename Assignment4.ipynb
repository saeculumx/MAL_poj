{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#read data\u001B[39;00m\n\u001B[0;32m      2\u001B[0m data_set_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./4 The Local Elections\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 3\u001B[0m x_horsens \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{}\u001B[39;49;00m\u001B[38;5;124;43m/X_Horsens.npy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_set_path\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m y_horsens \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/Y_Horsens.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(data_set_path))\n\u001B[0;32m      5\u001B[0m x_databorg \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m/X_Databorg.npy\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(data_set_path))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:435\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001B[0m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    433\u001B[0m     \u001B[38;5;66;03m# Try a pickle\u001B[39;00m\n\u001B[0;32m    434\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m allow_pickle:\n\u001B[1;32m--> 435\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot load file containing pickled data \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    436\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhen allow_pickle=False\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    437\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    438\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m pickle\u001B[38;5;241m.\u001B[39mload(fid, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpickle_kwargs)\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "data_set_path = \"./4 The Local Elections\"\n",
    "x_horsens = np.load(\"{}/X_Horsens.npy\".format(data_set_path))\n",
    "y_horsens = np.load(\"{}/Y_Horsens.npy\".format(data_set_path))\n",
    "x_databorg = np.load(\"{}/X_Databorg.npy\".format(data_set_path))\n",
    "y_databorg = np.load(\"{}/Y_Databorg.npy\".format(data_set_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(a)__ How well do you (intuitively) expect that we can predict the partisan affiliation of a candidate based on their answers to the test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can get clear predict which has very clear boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__(b)__ Based on the answers from all 83 candidates for the Horsens city council, perform a Principal Component Analysis with 2 principal components. Plot the results in a figure using these 2 components as the axes. Label the points with the party letter and the appropriate color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "pca_horsens = PCA(n_components=2)\n",
    "x_horsens_transform = pca_horsens.fit_transform(x_horsens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#political parties\n",
    "label_dict = {}\n",
    "colors_horsens = []\n",
    "for label in y_horsens:\n",
    "    if label not in label_dict.keys():\n",
    "        label_dict[label] = len(label_dict)\n",
    "    colors_horsens.append(label_dict[label])\n",
    "colors = np.round(np.array(colors_horsens)/(len(label_dict)-1)*100)\n",
    "print(label_dict)\n",
    "plt.scatter(x_horsens_transform[:,0],x_horsens_transform[:,1],c=colors_horsens, cmap='tab20')\n",
    "plt.title(\"political parties (horsens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__(c)__ Comment on the results. You may consider the following questions for inspiration: Can the political parties be separated? Can the typical distinction of \"left-wing\" and \"right-wing\" be discerned? Which of the 18 questions (features) are most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#political position (left:0,right:1)\n",
    "position_dict = {'A': 0, 'B': 0, 'C': 1, 'D': 1, 'F': 0, 'I': 1, 'O': 1, 'V': 1, 'Z': 0}\n",
    "colors = []\n",
    "for label in y_horsens:\n",
    "    colors.append(position_dict[label])\n",
    "colors = np.round(np.array(colors)/(len(label_dict)-1)*100)\n",
    "print(position_dict)\n",
    "plt.scatter(x_horsens_transform[:,0],x_horsens_transform[:,1],c=colors, cmap='tab20')\n",
    "plt.title(\"political position (horsens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#get max difference in mean\n",
    "importance_list = np.diagonal(pca_horsens.get_covariance())\n",
    "importance_pca = np.argmax(importance_list)+1\n",
    "print(importance_list)\n",
    "print(\"question {} is the most important one\".format(importance_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The political parties can be separated, but not very clear.\n",
    "\n",
    "The typical distinction of \"left-wing\" and \"right-wing\" can be discerned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__(d)__ Once again, perform a Principal Component Analysis and visualize the results. Compare the results to those of the Horsens data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pca_databorg = PCA(n_components=2)\n",
    "x_databorg_transform = pca_databorg.fit_transform(x_databorg)\n",
    "\n",
    "label_dict = {}\n",
    "colors = []\n",
    "for label in y_databorg:\n",
    "    if label not in label_dict.keys():\n",
    "        label_dict[label] = len(label_dict)\n",
    "    colors.append(label_dict[label])\n",
    "colors = np.round(np.array(colors)/(len(label_dict)-1)*100)\n",
    "print(label_dict)\n",
    "plt.scatter(x_databorg_transform[:,0],x_databorg_transform[:,1],c=colors, cmap='tab20')\n",
    "plt.title(\"political parties (databorg)\")\n",
    "plt.show()\n",
    "plt.scatter(x_horsens_transform[:,0],x_horsens_transform[:,1],c=colors_horsens, cmap='tab20')\n",
    "plt.title(\"political parties (horsens)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The feature area of each partisan is more clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "__(e)__ Split the data into a training and a validation set, with appropriate fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_set = []\n",
    "validation_set = []\n",
    "training_label = []\n",
    "validation_label = []\n",
    "label_dict = {}\n",
    "for i,label in enumerate(y_databorg):\n",
    "    if label not in label_dict.keys():\n",
    "        label_dict[label] = 0\n",
    "    label_dict[label] += 1\n",
    "training_num = {}\n",
    "for label in label_dict.keys():\n",
    "    training_num[label] = round(label_dict[label]*0.75)\n",
    "for i,label in enumerate(y_databorg):\n",
    "    data = x_databorg[i:i+1,:]+1.5\n",
    "    if training_num[label]>0:\n",
    "        training_set.append(data)\n",
    "        training_label.append(np.array([label]))\n",
    "        training_num[label] -= 1\n",
    "    else:\n",
    "        validation_set.append(data)\n",
    "        validation_label.append(np.array([label]))\n",
    "training_set = np.concatenate(training_set,axis=0)\n",
    "validation_set = np.concatenate(validation_set,axis=0)\n",
    "training_label = np.concatenate(training_label,axis=0)\n",
    "validation_label = np.concatenate(validation_label,axis=0)\n",
    "print(\"training: {}(75%) | validation: {}(25%)\".format(len(training_set),len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(f)__ Comment on the basic assumption of the Naive Bayes approach. Is this a reasonable assumption for the problem at hand?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The basic assumption of the Naive Bayes approach ask every feature is independent.\n",
    "It's not a reasonable assumption for the problem, because not each question is independent, some of them have relationship."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(g)__ Classify the instances of the validation set using a Naive Bayes approach. Comment on the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(training_set,training_label)\n",
    "validation_output = model_nb.predict(validation_set)\n",
    "validation_result = np.where(validation_label==validation_output,1,0)\n",
    "tpr_nb = validation_result.sum()/validation_result.shape[0]\n",
    "print(\"[Naive Bayes Categorical] TPR:{:.2%}\".format(tpr_nb))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(h)__ Using default settings of the _k_-NN classifier, classify the instances of the validation set. Comment on the performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "model_knn = knn(n_jobs=-1)\n",
    "model_knn.fit(training_set,training_label)\n",
    "tpr = model_knn.score(validation_set,validation_label)\n",
    "print(\"[KNN Classifier] K:{} TPR:{:.2%}\".format(model_knn.n_neighbors,tpr))\n",
    "print(\"The performance of KNN can higher than Naive Bayes\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(i)__ Play around with different values of _k_. Decide on a \"good\" value of _k_. Comment on the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_k = 150\n",
    "tpr_list = []\n",
    "for i in range(1,max_k+1):\n",
    "    model_knn.n_neighbors = i\n",
    "    tpr = model_knn.score(validation_set,validation_label)\n",
    "    tpr_list.append(tpr)\n",
    "best_k = np.argmax(tpr_list)+1\n",
    "tpr_knn = tpr_list[best_k-1]\n",
    "print(\"[KNN Classifier] K:{} TPR:{:.2%}\".format(best_k,tpr_knn))\n",
    "plt.plot(range(1,max_k+1),tpr_list)\n",
    "plt.title(\"TPR of K\")\n",
    "plt.show()\n",
    "print(\"The max TPR of KNN classifier is {:.2%}\".format(tpr_knn))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(j)__ What is the _minimum_ depth of an appropriate decision tree? Why?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are 18 questions, it means 18 features, so the maximum depth of appropriate decision tree should be 18."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(k)__ Build a decision tree with at least the depth from above. Play around with the tree depth. Include a figure that shows some relevant measure of the performance as a function of the tree depth. Comment on any issues of over-fitting. Decide on a tree which you will keep for later use. Can you do better than the _k_-NN classifier?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model_dt = DecisionTreeClassifier()\n",
    "model_dt.fit(training_set,training_label)\n",
    "tpr = model_dt.score(validation_set,validation_label)\n",
    "print(\"[Decision Tree Classifier] depth:{} TPR:{:.2%}\".format(model_dt.get_depth(),tpr))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_depth = 18\n",
    "start_depth = 1\n",
    "tpr_list = []\n",
    "training_tpr_list = []\n",
    "for i in range(start_depth,max_depth+1):\n",
    "    model_dt = DecisionTreeClassifier(max_depth=i)\n",
    "    model_dt.fit(training_set,training_label)\n",
    "    tpr = model_dt.score(validation_set,validation_label)\n",
    "    tpr_list.append(tpr)\n",
    "    training_tpr = model_dt.score(training_set,training_label)\n",
    "    training_tpr_list.append(training_tpr)\n",
    "best_depth = np.argmax(tpr_list)+start_depth\n",
    "tpr_dt = tpr_list[best_depth-start_depth]\n",
    "print(\"[Decision Tree Classifier] best_depth:{} TPR:{:.2%}\".format(best_depth,tpr_dt))\n",
    "plt.plot(range(start_depth,max_depth+1),tpr_list)\n",
    "plt.title(\"TPR of depth\")\n",
    "plt.show()\n",
    "print(\"The performance of Decision Tree is lower than KNN\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#over-fitting\n",
    "plt.plot(range(start_depth,max_depth+1),training_tpr_list)\n",
    "plt.plot(range(start_depth,max_depth+1),tpr_list)\n",
    "plt.title(\"training/validation TPR of depth\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The TPR on training set is rising with growing depth, but on the validation set, the TPR growth up first and then drop, that means the model is over-fitting. We can use pre-pruning or post-pruning to avoid the over-fitting. Use a suitable depth to stop build the tree early is a type of pre-pruning, and the post-pruning need build whole decision tree first:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pruning decision tree\n",
    "model_dt = DecisionTreeClassifier()\n",
    "print(\"[Decision Tree Classifier] TPR:{:.2%} depth:{}\".format(tpr_dt,best_depth))\n",
    "ccp_alphas = model_dt.cost_complexity_pruning_path(training_set,training_label)[\"ccp_alphas\"]\n",
    "ccp_alphas = np.unique(ccp_alphas)\n",
    "tpr_list = []\n",
    "training_tpr_list = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    model_dt = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    model_dt.fit(training_set,training_label)\n",
    "    tpr = model_dt.score(validation_set,validation_label)\n",
    "    tpr_list.append(tpr)\n",
    "    training_tpr = model_dt.score(training_set,training_label)\n",
    "    training_tpr_list.append(training_tpr)\n",
    "best_ccp_alpha = ccp_alphas[np.argmax(tpr_list)]\n",
    "tpr_dt_pruning = np.max(tpr_list)\n",
    "model_dt = DecisionTreeClassifier(ccp_alpha=best_ccp_alpha)\n",
    "model_dt.fit(training_set,training_label)\n",
    "print(\"[Decision Tree Classifier] TPR:{:.2%} depth:{} best_ccp_alpha:{}\".format(tpr_dt_pruning,model_dt.get_depth(),best_ccp_alpha))\n",
    "plt.plot(ccp_alphas,training_tpr_list)\n",
    "plt.plot(ccp_alphas,tpr_list)\n",
    "plt.title(\"TPR of ccp_alpha\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pre-pruning use very less time, but it will lead to under-fitting. The post-pruning will not lead to under-fitting easily, but it will cost more time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(l)__ What are the most important features? Visualize this in an appropriate way. Does it match what you would expect? Compare to the results of the PCA analysis. Do we expect them to be the same? Why/why not?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importance_list = model_dt.feature_importances_\n",
    "importance_dt = np.argmax(importance_list)+1\n",
    "plt.plot(range(1,importance_list.shape[0]+1),importance_list)\n",
    "plt.title(\"importance of feature\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most important feature is the feature (question) 10, it matches my expect."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text = \"is same as\"\n",
    "if importance_dt != importance_pca:\n",
    "    text = \"is not same as\"\n",
    "print(\"The result {} the result of the PCA analysis. I hope they are same, because that means the model calculate the result base on the most important feature.\".format(text))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(m)__ Build a decision forest. Play around with the number of trees in the forest. Decide on a forest."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#find best tree num\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "tpr_list = []\n",
    "tree_num_list = []\n",
    "max_tree_num = 151\n",
    "tree_num_step = 5\n",
    "tree_num_min_step = 1\n",
    "tree_num_range = [1,max_tree_num]\n",
    "while tree_num_step>=tree_num_min_step:\n",
    "    t_l = []\n",
    "    t_n_l = []\n",
    "    for tree_num in range(tree_num_range[0],tree_num_range[1]+1,tree_num_step):\n",
    "        model_rf = RandomForestClassifier(n_estimators=tree_num,n_jobs=-1)\n",
    "        model_rf.fit(training_set,training_label)\n",
    "        validation_output = model_rf.predict(validation_set)\n",
    "        validation_result = np.where(validation_label==validation_output,1,0)\n",
    "        tpr = validation_result.sum()/validation_result.shape[0]\n",
    "        t_l.append(tpr)\n",
    "        t_n_l.append(tree_num)\n",
    "    tpr_list.append(t_l)\n",
    "    tree_num_list.append(t_n_l)\n",
    "    max_tpr_tree_num = t_n_l[np.argmax(t_l)]\n",
    "    print(\"range:[{},{}] step:{} tree_num:{} TPR:{:.2%}\".format(tree_num_range[0],tree_num_range[1],tree_num_step,max_tpr_tree_num,np.max(t_l)))\n",
    "    tree_num_range = [max_tpr_tree_num-tree_num_step*2,max_tpr_tree_num+tree_num_step*2]\n",
    "    tree_num_step = int(tree_num_step/2)\n",
    "best_tree_num = tree_num_list[-1][np.argmax(tpr_list[-1])]\n",
    "tpr_rf = np.max(tpr_list[-1])\n",
    "print(\"[Random Forest Classifier] best_tree_num:{} TPR:{:.2%}\".format(best_tree_num,tpr_rf))\n",
    "for i in range(len(tpr_list)):\n",
    "    plt.plot(tree_num_list[i],tpr_list[i])\n",
    "plt.title(\"TPR of tree number\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#find best depth\n",
    "max_depth = 18\n",
    "start_depth = 1\n",
    "tpr_list = []\n",
    "for i in range(start_depth,max_depth+1):\n",
    "    model_rf = RandomForestClassifier(n_estimators=best_tree_num,max_depth=i,n_jobs=-1)\n",
    "    model_rf.fit(training_set,training_label)\n",
    "    validation_output = model_rf.predict(validation_set)\n",
    "    validation_result = np.where(validation_label==validation_output,1,0)\n",
    "    tpr = validation_result.sum()/validation_result.shape[0]\n",
    "    tpr_list.append(tpr)\n",
    "best_depth_rf = np.argmax(tpr_list)+start_depth\n",
    "tpr_rf = tpr_list[np.argmax(tpr_list)]\n",
    "print(\"[Random Forest Classifier] best_tree_num:{} best_depth:{} TPR:{:.2%}\".format(best_tree_num,best_depth_rf,tpr_rf))\n",
    "plt.plot(range(start_depth,max_depth+1),tpr_list)\n",
    "plt.title(\"TPR of depth\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(n)__ Extract the most important features. Comment and compare with previously obtained results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=best_tree_num,max_depth=best_depth_rf,n_jobs=-1)\n",
    "model_rf.fit(training_set,training_label)\n",
    "importance_list = model_rf.feature_importances_\n",
    "importance_rf = np.argmax(importance_list)+1\n",
    "plt.plot(range(1,importance_list.shape[0]+1),importance_list)\n",
    "plt.title(\"importance of feature\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The most important feature is the feature (question) 10, it is the same result as other models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "__(o)__ Compare the results of the in terms of confusion matrices, accuracy, precision, recall, and f-score. How well can we predict the partisan affiliation of a candidate based on their answers to a test? How does this compare with your intuition?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_nb = CategoricalNB()\n",
    "model_nb.fit(training_set,training_label)\n",
    "model_knn = knn(n_neighbors=best_k,n_jobs=-1)\n",
    "model_knn.fit(training_set,training_label)\n",
    "model_dt = DecisionTreeClassifier(ccp_alpha=best_ccp_alpha)\n",
    "model_dt.fit(training_set,training_label)\n",
    "model_rf = RandomForestClassifier(n_estimators=best_tree_num,max_depth=best_depth_rf,n_jobs=-1)\n",
    "model_rf.fit(training_set,training_label)\n",
    "model_list = [model_nb,model_knn,model_dt,model_rf]\n",
    "model_name = [\"Naive Bayes Classifier\",\"KNN Classifier\",\"Decision Tree Classifier\",\"Random Forest Classifier\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for model in model_list:\n",
    "    validation_output = model.predict(validation_set)\n",
    "    confusion_matrix = np.zeros([len(label_dict),len(label_dict)])\n",
    "    for l_i,label in enumerate(label_dict.keys()):\n",
    "        tp = np.sum(np.where((validation_label==label) & (validation_output==label),1,0))\n",
    "        for l_fp_i,label_fp in enumerate(label_dict.keys()):\n",
    "            if label!= label_fp:\n",
    "                fp = np.sum(np.where((validation_label==label_fp) & (validation_output==label),1,0))\n",
    "                confusion_matrix[l_i,l_fp_i] = fp\n",
    "            else:\n",
    "                confusion_matrix[l_i,l_fp_i] = tp\n",
    "    aprf_list = []\n",
    "    tp_model = 0\n",
    "    fp_model = 0\n",
    "    fn_model = 0\n",
    "    tn_model = 0\n",
    "    for l_i,label in enumerate(label_dict.keys()):\n",
    "        tp = confusion_matrix[l_i,l_i]\n",
    "        fp = np.sum(confusion_matrix[l_i,:]) - tp\n",
    "        fn = np.sum(confusion_matrix[:,l_i]) - tp\n",
    "        tn = np.sum(confusion_matrix) - tp - fp - fn\n",
    "        tp_model += tp\n",
    "        fp_model += fp\n",
    "        fn_model += fn\n",
    "        tn_model += tn\n",
    "        accuracy = (tp+tn)/(tp+fp+fn+tn)\n",
    "        precision = tp / (tp+fp)\n",
    "        recall =tp / (tp+fn)\n",
    "        f_score = 2*precision * recall / (precision + recall)\n",
    "        aprf_list.append([accuracy,precision,recall,f_score])\n",
    "    accuracy_model = (tp_model+tn_model)/(tp_model+fp_model+fn_model+tn_model)\n",
    "    precision_model = tp_model / (tp_model+fp_model)\n",
    "    recall_model =tp_model / (tp_model+fn_model)\n",
    "    f_score_model = 2*precision_model * recall_model / (precision_model + recall_model)\n",
    "    result_list.append([confusion_matrix,accuracy_model,precision_model,recall_model,f_score_model,aprf_list])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for r_i,[confusion_matrix,accuracy_model,precision_model,recall_model,f_score_model,aprf_list] in enumerate(result_list):\n",
    "    print(\"{}\\n[model] accuracy:{:.2%} precision:{:.2%} recall:{:.2%} f_score:{}\".format(model_name[r_i],accuracy_model,precision_model,recall_model,f_score_model))\n",
    "    for i,[accuracy,precision,recall,f_score] in enumerate(aprf_list):\n",
    "        label = list(label_dict.keys())[i]\n",
    "        print(\"{} accuracy:{:.2%} precision:{:.2%} recall:{:.2%} f_score:{}\".format(label,accuracy,precision,recall,f_score))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The f-score of models are nearly 0.8, so we can predict the partisan affiliation of most candidates(80%) based on their answers to a test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}